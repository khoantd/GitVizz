"""
Graph Search Tool for GitVizz

This module provides a comprehensive set of tools for searching, filtering, and extracting
information from code graphs generated by GitVizz. All methods return GraphGenerator 
subgraphs that can be directly visualized and chained.
"""

import re
from typing import Dict, List, Any, Optional, Union
import networkx as nx
from difflib import SequenceMatcher
from collections import defaultdict, deque

# Import graph data structures
from .graph_generator import GraphGenerator, GraphNodeData, GraphEdgeData


class GraphSearchTool:
    """
    A comprehensive tool for searching and analyzing code graphs.
    
    All methods return GraphGenerator subgraphs that can be:
    - Directly visualized with .visualize()
    - Chained with other GraphSearchTool operations
    - Used for LLM context generation
    """
    
    def __init__(self, graph_generator: GraphGenerator):
        """
        Initialize the search tool with a graph generator.
        
        Args:
            graph_generator: A GraphGenerator instance with parsed graph data
        """
        self.graph_generator = graph_generator
        self.graph: nx.DiGraph = None
        self.nodes_map: Dict[str, GraphNodeData] = {}
        self.edges_list: List[GraphEdgeData] = []
        
        self._load_graph_data()
    
    def _load_graph_data(self):
        """Load and prepare graph data for searching."""
        # Ensure graph is generated
        if not self.graph_generator.all_nodes_data:
            self.graph_generator._generate_graph_elements()
        
        # Convert to NetworkX for advanced operations
        self.graph = self.graph_generator.to_networkx()
        
        # Create lookup maps
        self.nodes_map = {node["id"]: node for node in self.graph_generator.all_nodes_data}
        self.edges_list = self.graph_generator.all_edges_data
    
    def _create_subgraph_generator(self, subgraph_data: Dict[str, Any]) -> GraphGenerator:
        """Create a new GraphGenerator instance from subgraph data."""
        # Create empty GraphGenerator
        subgraph_gen = GraphGenerator([])
        
        # Set the subgraph data
        subgraph_gen.all_nodes_data = subgraph_data["nodes"]
        subgraph_gen.all_edges_data = subgraph_data["edges"]
        subgraph_gen.node_details_map = {node["id"]: node for node in subgraph_data["nodes"]}
        
        # Copy project metadata
        subgraph_gen.project_type = getattr(self.graph_generator, 'project_type', 'unknown')
        subgraph_gen.project_root_path = getattr(self.graph_generator, 'project_root_path', '.')
        
        # Add search metadata as attribute
        if "metadata" in subgraph_data:
            subgraph_gen._search_metadata = subgraph_data["metadata"]
        
        return subgraph_gen
    
    def _extract_subgraph_data(self, node_ids: List[str], depth: int = 1, direction: str = "both") -> Dict[str, Any]:
        """Extract subgraph data around specified nodes."""
        if not self.graph:
            return {"nodes": [], "edges": []}
        
        subgraph_nodes = set(node_ids)
        
        # Traverse the graph to find connected nodes
        for node_id in node_ids:
            if node_id not in self.graph:
                continue
            
            visited = set()
            queue = deque([(node_id, 0)])
            
            while queue:
                current_node, current_depth = queue.popleft()
                
                if current_depth >= depth or current_node in visited:
                    continue
                
                visited.add(current_node)
                subgraph_nodes.add(current_node)
                
                # Add neighbors based on direction
                if direction in ["out", "both"]:
                    for neighbor in self.graph.successors(current_node):
                        if neighbor not in visited:
                            queue.append((neighbor, current_depth + 1))
                
                if direction in ["in", "both"]:
                    for neighbor in self.graph.predecessors(current_node):
                        if neighbor not in visited:
                            queue.append((neighbor, current_depth + 1))
        
        # Extract subgraph data
        subgraph_node_data = [
            self.nodes_map[node_id] for node_id in subgraph_nodes 
            if node_id in self.nodes_map
        ]
        
        subgraph_edges = [
            edge for edge in self.edges_list
            if edge["source"] in subgraph_nodes and edge["target"] in subgraph_nodes
        ]
        
        return {
            "nodes": subgraph_node_data,
            "edges": subgraph_edges
        }
    
    def _calculate_similarity(self, query: str, text: str) -> float:
        """Calculate similarity between query and text using SequenceMatcher."""
        if not query or not text:
            return 0.0
        
        # Use SequenceMatcher for basic similarity
        similarity = SequenceMatcher(None, query, text).ratio()
        
        # Bonus for partial matches
        if query in text:
            similarity = max(similarity, 0.6)
        
        # Bonus for word boundary matches
        if re.search(r'\b' + re.escape(query) + r'\b', text, re.IGNORECASE):
            similarity = max(similarity, 0.8)
        
        return similarity
    
    def _get_match_type(self, query: str, node_data: Dict[str, Any], node_id: str) -> str:
        """Determine the type of match found."""
        name = node_data.get("name", "").lower()
        code = node_data.get("code", "").lower() if node_data.get("code") else ""
        
        if query in name:
            return "name_match"
        elif query in node_id.lower():
            return "id_match"
        elif query in code:
            return "code_match"
        else:
            return "fuzzy_match"
    
    def fuzzy_search(self, query: str, 
                    categories: Optional[List[str]] = None,
                    similarity_threshold: float = 0.3,
                    max_results: int = 20,
                    depth: int = 1) -> GraphGenerator:
        """
        Perform fuzzy search and return a subgraph.
        
        Args:
            query: Search query string
            categories: Optional list of node categories to filter by
            similarity_threshold: Minimum similarity score (0.0 to 1.0)
            max_results: Maximum number of results to return
            depth: Depth for subgraph extraction around matches
        
        Returns:
            GraphGenerator subgraph containing matches and their context
        """
        query_lower = query.lower()
        matching_nodes = []
        
        for node_id, node_data in self.nodes_map.items():
            # Skip if category filter is specified and doesn't match
            if categories and node_data.get("category") not in categories:
                continue
            
            # Calculate similarity scores for different fields
            name_similarity = self._calculate_similarity(query_lower, node_data.get("name", "").lower())
            id_similarity = self._calculate_similarity(query_lower, node_id.lower())
            
            # Check code content if available
            code_similarity = 0.0
            if node_data.get("code"):
                code_similarity = self._calculate_similarity(query_lower, node_data["code"].lower())
            
            # Take the maximum similarity
            max_similarity = max(name_similarity, id_similarity, code_similarity)
            
            # Check for exact substring matches (higher weight)
            if query_lower in node_data.get("name", "").lower():
                max_similarity = max(max_similarity, 0.8)
            if query_lower in node_id.lower():
                max_similarity = max(max_similarity, 0.7)
            
            # Add to results if above threshold
            if max_similarity >= similarity_threshold:
                matching_nodes.append({
                    "node_id": node_id,
                    "similarity_score": max_similarity,
                    "match_type": self._get_match_type(query_lower, node_data, node_id)
                })
        
        # Sort by similarity score (descending)
        matching_nodes.sort(key=lambda x: x["similarity_score"], reverse=True)
        center_nodes = [node["node_id"] for node in matching_nodes[:max_results]]
        
        # Extract subgraph around matching nodes
        subgraph_data = self._extract_subgraph_data(center_nodes, depth=depth)
        
        # Add search metadata to subgraph
        subgraph_data["metadata"] = {
            "search_query": query,
            "search_type": "fuzzy_search",
            "total_matches": len(matching_nodes),
            "similarity_threshold": similarity_threshold,
            "center_nodes": center_nodes,
            "match_details": matching_nodes[:max_results]
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def filter_by_category(self, categories: List[str], depth: int = 1) -> GraphGenerator:
        """
        Filter nodes by category and return subgraph.
        
        Args:
            categories: List of categories to include
            depth: Depth for subgraph extraction around matches
        
        Returns:
            GraphGenerator subgraph containing filtered nodes and their context
        """
        matching_nodes = [
            node_id for node_id, node_data in self.nodes_map.items()
            if node_data.get("category") in categories
        ]
        
        subgraph_data = self._extract_subgraph_data(matching_nodes, depth=depth)
        
        subgraph_data["metadata"] = {
            "search_type": "category_filter",
            "categories": categories,
            "center_nodes": matching_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_by_relationship(self, relationship_types: List[str], depth: int = 1) -> GraphGenerator:
        """
        Find nodes connected by specific relationship types.
        
        Args:
            relationship_types: List of relationship types to include
            depth: Depth for subgraph extraction
        
        Returns:
            GraphGenerator subgraph containing nodes with specified relationships
        """
        # Find edges with specified relationships
        matching_edges = [
            edge for edge in self.edges_list
            if edge.get("relationship") in relationship_types
        ]
        
        # Get all nodes involved in these relationships
        involved_nodes = set()
        for edge in matching_edges:
            involved_nodes.add(edge["source"])
            involved_nodes.add(edge["target"])
        
        subgraph_data = self._extract_subgraph_data(list(involved_nodes), depth=depth)
        
        subgraph_data["metadata"] = {
            "search_type": "relationship_filter",
            "relationship_types": relationship_types,
            "center_nodes": list(involved_nodes)
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def get_neighbors(self, node_id: str, 
                     direction: str = "both",
                     relationship_types: Optional[List[str]] = None,
                     depth: int = 1) -> GraphGenerator:
        """
        Get neighbors of a specific node as a subgraph.
        
        Args:
            node_id: Center node ID
            direction: Direction to traverse ("in", "out", "both")
            relationship_types: Filter by relationship types
            depth: Depth of traversal
        
        Returns:
            GraphGenerator subgraph containing the node and its neighbors
        """
        if node_id not in self.graph:
            # Return empty subgraph
            return self._create_subgraph_generator({"nodes": [], "edges": [], "metadata": {
                "search_type": "neighbors",
                "center_node": node_id,
                "error": "Node not found"
            }})
        
        neighbor_nodes = set([node_id])  # Include the center node
        
        # Get neighbors based on direction
        if direction in ["out", "both"]:
            for neighbor in self.graph.successors(node_id):
                if not relationship_types:
                    neighbor_nodes.add(neighbor)
                else:
                    edge_data = self.graph.edges[node_id, neighbor]
                    if edge_data.get("relationship") in relationship_types:
                        neighbor_nodes.add(neighbor)
        
        if direction in ["in", "both"]:
            for neighbor in self.graph.predecessors(node_id):
                if not relationship_types:
                    neighbor_nodes.add(neighbor)
                else:
                    edge_data = self.graph.edges[neighbor, node_id]
                    if edge_data.get("relationship") in relationship_types:
                        neighbor_nodes.add(neighbor)
        
        # Extract subgraph with additional depth if specified
        subgraph_data = self._extract_subgraph_data(list(neighbor_nodes), depth=depth)
        
        subgraph_data["metadata"] = {
            "search_type": "neighbors",
            "center_node": node_id,
            "direction": direction,
            "relationship_types": relationship_types,
            "neighbor_count": len(neighbor_nodes) - 1  # Exclude center node
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_paths(self, source_id: str, target_id: str, 
                   max_paths: int = 5, max_length: int = 6) -> GraphGenerator:
        """
        Find paths between two nodes and return as subgraph.
        
        Args:
            source_id: Source node ID
            target_id: Target node ID
            max_paths: Maximum number of paths to find
            max_length: Maximum path length
        
        Returns:
            GraphGenerator subgraph containing all nodes and edges in found paths
        """
        path_nodes = set()
        found_paths = []
        
        if not self.graph or source_id not in self.graph or target_id not in self.graph:
            return self._create_subgraph_generator({"nodes": [], "edges": [], "metadata": {
                "search_type": "find_paths",
                "source": source_id,
                "target": target_id,
                "error": "Source or target node not found"
            }})
        
        try:
            # Find all simple paths
            all_paths = list(nx.all_simple_paths(
                self.graph, source_id, target_id, cutoff=max_length
            ))
            
            # Sort by path length and take the best ones
            all_paths.sort(key=len)
            found_paths = all_paths[:max_paths]
            
            # Collect all nodes in paths
            for path in found_paths:
                path_nodes.update(path)
                
        except nx.NetworkXNoPath:
            pass
        
        # Extract subgraph containing path nodes
        subgraph_data = self._extract_subgraph_data(list(path_nodes), depth=0)
        
        subgraph_data["metadata"] = {
            "search_type": "find_paths",
            "source": source_id,
            "target": target_id,
            "paths_found": found_paths,
            "path_count": len(found_paths)
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def get_connected_component(self, node_id: str) -> GraphGenerator:
        """
        Get the entire connected component containing a node.
        
        Args:
            node_id: Node ID to find component for
        
        Returns:
            GraphGenerator subgraph containing the connected component
        """
        if not self.graph or node_id not in self.graph:
            return self._create_subgraph_generator({"nodes": [], "edges": [], "metadata": {
                "search_type": "connected_component",
                "center_node": node_id,
                "error": "Node not found"
            }})
        
        # Find weakly connected component (ignores edge direction)
        component_nodes = list(nx.node_connected_component(
            self.graph.to_undirected(), node_id
        ))
        
        subgraph_data = self._extract_subgraph_data(component_nodes, depth=0)
        
        subgraph_data["metadata"] = {
            "search_type": "connected_component",
            "center_node": node_id,
            "component_size": len(component_nodes)
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def get_high_connectivity_nodes(self, min_connections: int = 3, depth: int = 1) -> GraphGenerator:
        """
        Find nodes with high connectivity (potential complexity hotspots).
        
        Args:
            min_connections: Minimum number of connections required
            depth: Depth for subgraph extraction
        
        Returns:
            GraphGenerator subgraph containing high-connectivity nodes and their context
        """
        high_connectivity_nodes = []
        
        for node_id in self.graph.nodes():
            # Count both incoming and outgoing connections
            total_connections = self.graph.degree(node_id)
            if total_connections >= min_connections:
                high_connectivity_nodes.append({
                    "node_id": node_id,
                    "connections": total_connections
                })
        
        # Sort by connection count
        high_connectivity_nodes.sort(key=lambda x: x["connections"], reverse=True)
        center_nodes = [node["node_id"] for node in high_connectivity_nodes]
        
        subgraph_data = self._extract_subgraph_data(center_nodes, depth=depth)
        
        subgraph_data["metadata"] = {
            "search_type": "high_connectivity",
            "min_connections": min_connections,
            "center_nodes": center_nodes,
            "connectivity_details": high_connectivity_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    @staticmethod
    def build_llm_context(subgraphs: Union[GraphGenerator, List[GraphGenerator]], 
                         include_code: bool = True,
                         max_code_length: int = 500,
                         include_relationships: bool = True,
                         context_type: str = "analysis") -> str:
        """
        Build LLM context from one or more subgraphs.
        
        Args:
            subgraphs: Single GraphGenerator or list of GraphGenerators
            include_code: Whether to include code snippets
            max_code_length: Maximum length of code snippets
            include_relationships: Whether to include relationship information
            context_type: Type of context ("analysis", "review", "security", "refactoring")
        
        Returns:
            Formatted string for LLM context
        """
        if isinstance(subgraphs, GraphGenerator):
            subgraphs = [subgraphs]
        
        context_parts = []
        
        # Add context type header
        if context_type == "review":
            context_parts.append("# Code Review Analysis")
            context_parts.append("This analysis identifies potential issues and improvement opportunities.")
        elif context_type == "security":
            context_parts.append("# Security Analysis")
            context_parts.append("This analysis focuses on security-sensitive code areas.")
        elif context_type == "refactoring":
            context_parts.append("# Refactoring Opportunities")
            context_parts.append("This analysis identifies code that may benefit from refactoring.")
        else:
            context_parts.append("# Code Analysis")
        
        context_parts.append("")
        
        for i, subgraph in enumerate(subgraphs):
            if hasattr(subgraph, '_search_metadata'):
                metadata = subgraph._search_metadata
                context_parts.append(f"## Search Result {i+1}: {metadata.get('search_type', 'unknown')}")
                if 'search_query' in metadata:
                    context_parts.append(f"Query: '{metadata['search_query']}'")
                context_parts.append(f"Nodes: {len(subgraph.all_nodes_data)}, Edges: {len(subgraph.all_edges_data)}")
                
                # Add metadata insights
                if metadata.get('search_type') == 'anti_patterns':
                    instances = metadata.get('instances_found', 0)
                    if instances > 0:
                        context_parts.append(f"âš ï¸ Found {instances} instances of {metadata.get('pattern_type', 'unknown')} anti-pattern")
                elif metadata.get('search_type') == 'security_hotspots':
                    hotspots = metadata.get('hotspots_found', 0)
                    if hotspots > 0:
                        context_parts.append(f"ðŸ”’ Found {hotspots} security-sensitive areas")
                elif metadata.get('search_type') == 'test_coverage_gaps':
                    coverage = metadata.get('coverage_ratio', 0)
                    context_parts.append(f"ðŸ§ª Test coverage: {coverage:.1%}")
                
                context_parts.append("")
            
            # Process each node in the subgraph
            for node in subgraph.all_nodes_data:
                module_id = node.get("id", "unknown")
                file_path = node.get("file", "unknown")
                name = node.get("name", "unknown")
                category = node.get("category", "unknown")
                start_line = node.get("start_line", 0)
                end_line = node.get("end_line", 0)
                code = node.get("code", "") if include_code else ""
                
                # Build the context block
                context_block = f"Module {{{module_id}}}\nFile: {file_path}\nDefines:\n"
                context_block += f"\n{name} ({category}) â€” lines {start_line}â€“{end_line}\n"
                
                # Add relationships if requested
                if include_relationships:
                    node_relationships = [
                        edge for edge in subgraph.all_edges_data 
                        if edge.get("source") == module_id or edge.get("target") == module_id
                    ]
                    for rel in node_relationships:
                        rel_type = rel.get("relationship", "unknown")
                        context_block += f"Relationship: {rel['source']} â†’ {rel['target']} ({rel_type})\n"
                
                # Add code if available and requested
                if code and include_code:
                    # Truncate code if too long
                    if len(code) > max_code_length:
                        code = code[:max_code_length] + "..."
                    
                    context_block += f"\nCode:\n\n```\n{code}\n```\n"
                
                context_parts.append(context_block)
                context_parts.append("\\")  # Separator as requested
            
            context_parts.append("")  # Extra space between subgraphs
        
        return "\n".join(context_parts)
    
    def find_similar_structures(self, pattern_subgraph: GraphGenerator, 
                               similarity_threshold: float = 0.7) -> GraphGenerator:
        """
        Find code structures similar to a given pattern.
        
        Args:
            pattern_subgraph: GraphGenerator containing the pattern to match
            similarity_threshold: Minimum similarity score for matches
        
        Returns:
            GraphGenerator subgraph containing similar structures
        """
        pattern_nodes = {node["category"] for node in pattern_subgraph.all_nodes_data}
        pattern_edges = {edge["relationship"] for edge in pattern_subgraph.all_edges_data}
        
        similar_nodes = []
        
        # Find nodes with similar patterns
        for node_id, node_data in self.nodes_map.items():
            if node_data.get("category") not in pattern_nodes:
                continue
                
            # Get local neighborhood
            local_neighbors = list(self.graph.successors(node_id)) + list(self.graph.predecessors(node_id))
            local_categories = {self.nodes_map[n]["category"] for n in local_neighbors if n in self.nodes_map}
            local_relationships = set()
            
            for neighbor in local_neighbors:
                if neighbor in self.nodes_map:
                    if self.graph.has_edge(node_id, neighbor):
                        local_relationships.add(self.graph.edges[node_id, neighbor].get("relationship", ""))
                    if self.graph.has_edge(neighbor, node_id):
                        local_relationships.add(self.graph.edges[neighbor, node_id].get("relationship", ""))
            
            # Calculate similarity
            category_similarity = len(pattern_nodes & local_categories) / max(len(pattern_nodes), 1)
            relationship_similarity = len(pattern_edges & local_relationships) / max(len(pattern_edges), 1)
            overall_similarity = (category_similarity + relationship_similarity) / 2
            
            if overall_similarity >= similarity_threshold:
                similar_nodes.append({
                    "node_id": node_id,
                    "similarity_score": overall_similarity
                })
        
        # Sort by similarity
        similar_nodes.sort(key=lambda x: x["similarity_score"], reverse=True)
        center_nodes = [node["node_id"] for node in similar_nodes]
        
        subgraph_data = self._extract_subgraph_data(center_nodes, depth=1)
        subgraph_data["metadata"] = {
            "search_type": "similar_structures",
            "pattern_nodes": len(pattern_subgraph.all_nodes_data),
            "matches_found": len(similar_nodes),
            "similarity_threshold": similarity_threshold,
            "center_nodes": center_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_unused_code(self, min_depth: int = 2) -> GraphGenerator:
        """
        Find potentially unused code (nodes with no incoming connections).
        
        Args:
            min_depth: Minimum depth from entry points to consider unused
        
        Returns:
            GraphGenerator subgraph containing potentially unused code
        """
        # Find entry points (modules, main functions)
        entry_points = [
            node_id for node_id, node_data in self.nodes_map.items()
            if node_data.get("category") in ["module"] or
               (node_data.get("name") in ["main", "__main__", "run", "start"])
        ]
        
        # Find all reachable nodes from entry points
        reachable = set()
        for entry in entry_points:
            if entry in self.graph:
                reachable.update(nx.descendants(self.graph, entry))
                reachable.add(entry)
        
        # Find unreachable nodes (excluding certain categories)
        unreachable_nodes = []
        for node_id, node_data in self.nodes_map.items():
            if (node_id not in reachable and 
                node_data.get("category") not in ["directory", "external_symbol"]):
                
                # Check if truly isolated
                incoming = len(list(self.graph.predecessors(node_id)))
                if incoming == 0:
                    unreachable_nodes.append(node_id)
        
        subgraph_data = self._extract_subgraph_data(unreachable_nodes, depth=0)
        subgraph_data["metadata"] = {
            "search_type": "unused_code",
            "entry_points": entry_points,
            "reachable_count": len(reachable),
            "unused_count": len(unreachable_nodes),
            "center_nodes": unreachable_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_circular_dependencies(self) -> GraphGenerator:
        """
        Find circular dependencies in the code.
        
        Returns:
            GraphGenerator subgraph containing nodes involved in cycles
        """
        cycles = list(nx.simple_cycles(self.graph))
        cycle_nodes = set()
        
        for cycle in cycles:
            cycle_nodes.update(cycle)
        
        subgraph_data = self._extract_subgraph_data(list(cycle_nodes), depth=0)
        subgraph_data["metadata"] = {
            "search_type": "circular_dependencies",
            "cycles_found": cycles,
            "cycle_count": len(cycles),
            "nodes_in_cycles": len(cycle_nodes),
            "center_nodes": list(cycle_nodes)
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_anti_patterns(self, pattern_type: str = "god_class") -> GraphGenerator:
        """
        Find common anti-patterns in the code.
        
        Args:
            pattern_type: Type of anti-pattern ('god_class', 'long_method', 'deep_nesting')
        
        Returns:
            GraphGenerator subgraph containing anti-pattern instances
        """
        anti_pattern_nodes = []
        
        if pattern_type == "god_class":
            # Find classes with too many methods/high connectivity
            for node_id, node_data in self.nodes_map.items():
                if node_data.get("category") == "class":
                    methods = [n for n in self.graph.successors(node_id) 
                             if n in self.nodes_map and self.nodes_map[n].get("category") == "method"]
                    if len(methods) > 10:  # Threshold for "god class"
                        anti_pattern_nodes.append({
                            "node_id": node_id,
                            "issue_count": len(methods)
                        })
        
        elif pattern_type == "long_method":
            # Find methods with too many lines
            for node_id, node_data in self.nodes_map.items():
                if node_data.get("category") == "method":
                    start_line = node_data.get("start_line", 0)
                    end_line = node_data.get("end_line", 0)
                    line_count = end_line - start_line + 1
                    if line_count > 50:  # Threshold for "long method"
                        anti_pattern_nodes.append({
                            "node_id": node_id,
                            "issue_count": line_count
                        })
        
        # Sort by severity (issue count)
        anti_pattern_nodes.sort(key=lambda x: x["issue_count"], reverse=True)
        center_nodes = [node["node_id"] for node in anti_pattern_nodes]
        
        subgraph_data = self._extract_subgraph_data(center_nodes, depth=1)
        subgraph_data["metadata"] = {
            "search_type": "anti_patterns",
            "pattern_type": pattern_type,
            "instances_found": len(anti_pattern_nodes),
            "pattern_details": anti_pattern_nodes,
            "center_nodes": center_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_interface_violations(self) -> GraphGenerator:
        """
        Find potential interface/abstraction violations.
        
        Returns:
            GraphGenerator subgraph containing violation instances
        """
        violation_nodes = []
        
        # Find classes that might violate single responsibility
        for node_id, node_data in self.nodes_map.items():
            if node_data.get("category") == "class":
                # Get all methods of this class
                methods = [n for n in self.graph.successors(node_id) 
                         if n in self.nodes_map and self.nodes_map[n].get("category") == "method"]
                
                # Analyze method relationships
                external_calls = 0
                internal_calls = 0
                
                for method in methods:
                    for call_target in self.graph.successors(method):
                        if call_target.startswith(node_id):  # Internal call
                            internal_calls += 1
                        else:
                            external_calls += 1
                
                # High external/internal ratio might indicate SRP violation
                if len(methods) > 0 and external_calls > internal_calls * 2:
                    violation_nodes.append({
                        "node_id": node_id,
                        "violation_score": external_calls / max(internal_calls, 1)
                    })
        
        violation_nodes.sort(key=lambda x: x["violation_score"], reverse=True)
        center_nodes = [node["node_id"] for node in violation_nodes]
        
        subgraph_data = self._extract_subgraph_data(center_nodes, depth=1)
        subgraph_data["metadata"] = {
            "search_type": "interface_violations",
            "violations_found": len(violation_nodes),
            "violation_details": violation_nodes,
            "center_nodes": center_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def get_dependency_layers(self) -> GraphGenerator:
        """
        Analyze architectural layers and dependencies.
        
        Returns:
            GraphGenerator subgraph showing layered architecture
        """
        # Calculate topological layers
        try:
            layers = list(nx.topological_generations(self.graph))
        except nx.NetworkXError:
            # Graph has cycles, use weakly connected components instead
            layers = list(nx.weakly_connected_components(self.graph))
        
        layer_nodes = []
        layer_info = []
        
        for i, layer in enumerate(layers):
            layer_nodes.extend(layer)
            layer_info.append({
                "layer": i,
                "nodes": list(layer),
                "count": len(layer)
            })
        
        subgraph_data = self._extract_subgraph_data(layer_nodes, depth=0)
        subgraph_data["metadata"] = {
            "search_type": "dependency_layers",
            "total_layers": len(layers),
            "layer_info": layer_info,
            "center_nodes": layer_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_test_coverage_gaps(self) -> GraphGenerator:
        """
        Find code that appears to lack test coverage.
        
        Returns:
            GraphGenerator subgraph containing untested code
        """
        # Find test-related nodes
        test_nodes = [
            node_id for node_id, node_data in self.nodes_map.items()
            if any(keyword in (node_data.get("file", "") or "").lower() 
                  for keyword in ["test", "spec"]) or
               any(keyword in (node_data.get("name", "") or "").lower() 
                  for keyword in ["test", "spec"])
        ]
        
        # Find nodes tested by test nodes
        tested_nodes = set()
        for test_node in test_nodes:
            if test_node in self.graph:
                tested_nodes.update(nx.descendants(self.graph, test_node))
        
        # Find untested nodes (excluding test nodes themselves)
        untested_nodes = [
            node_id for node_id, node_data in self.nodes_map.items()
            if (node_id not in tested_nodes and 
                node_id not in test_nodes and
                node_data.get("category") in ["function", "method", "class"])
        ]
        
        subgraph_data = self._extract_subgraph_data(untested_nodes, depth=1)
        subgraph_data["metadata"] = {
            "search_type": "test_coverage_gaps",
            "test_nodes": len(test_nodes),
            "tested_nodes": len(tested_nodes),
            "untested_nodes": len(untested_nodes),
            "coverage_ratio": len(tested_nodes) / max(len(self.nodes_map) - len(test_nodes), 1),
            "center_nodes": untested_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_entry_points(self) -> GraphGenerator:
        """
        Find application entry points (main functions, modules).
        
        Returns:
            GraphGenerator subgraph containing entry points and their immediate dependencies
        """
        entry_nodes = []
        
        for node_id, node_data in self.nodes_map.items():
            # Check for main functions
            if node_data.get("name") in ["main", "__main__", "run", "start", "init", "setup"]:
                entry_nodes.append(node_id)
            
            # Check for module-level entry points
            if node_data.get("category") == "module" and "main" in node_data.get("file", ""):
                entry_nodes.append(node_id)
            
            # Check for class constructors that might be entry points
            if (node_data.get("name") == "__init__" and 
                node_data.get("category") == "method" and
                "Application" in node_id):
                entry_nodes.append(node_id)
        
        subgraph_data = self._extract_subgraph_data(entry_nodes, depth=2)
        subgraph_data["metadata"] = {
            "search_type": "entry_points",
            "entry_points_found": len(entry_nodes),
            "center_nodes": entry_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_external_dependencies(self) -> GraphGenerator:
        """
        Find external dependencies and third-party imports.
        
        Returns:
            GraphGenerator subgraph containing external dependencies
        """
        external_nodes = []
        
        for node_id, node_data in self.nodes_map.items():
            # Find external symbols (third-party imports)
            if node_data.get("category") == "external_symbol":
                external_nodes.append(node_id)
            
            # Find import statements to external libraries
            if node_data.get("category") == "module":
                # Check if this looks like an external module
                module_name = node_data.get("name", "")
                if any(external in module_name.lower() for external in 
                      ["fastapi", "django", "flask", "requests", "pandas", "numpy", "jwt", "asyncpg"]):
                    external_nodes.append(node_id)
        
        subgraph_data = self._extract_subgraph_data(external_nodes, depth=1)
        subgraph_data["metadata"] = {
            "search_type": "external_dependencies",
            "external_count": len(external_nodes),
            "center_nodes": external_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_data_flow(self, start_node: str, end_node: str = None) -> GraphGenerator:
        """
        Trace data flow from a starting node.
        
        Args:
            start_node: Starting node ID
            end_node: Optional ending node ID
        
        Returns:
            GraphGenerator subgraph showing data flow paths
        """
        if start_node not in self.graph:
            return self._create_subgraph_generator({"nodes": [], "edges": [], "metadata": {
                "search_type": "data_flow",
                "error": "Start node not found"
            }})
        
        flow_nodes = set([start_node])
        
        if end_node and end_node in self.graph:
            # Find paths between start and end
            try:
                paths = list(nx.all_simple_paths(self.graph, start_node, end_node, cutoff=8))
                for path in paths[:5]:  # Limit to 5 paths
                    flow_nodes.update(path)
            except nx.NetworkXNoPath:
                pass
        else:
            # Trace forward from start node
            visited = set()
            queue = deque([(start_node, 0)])
            
            while queue:
                current, depth = queue.popleft()
                if depth > 4 or current in visited:  # Limit depth
                    continue
                
                visited.add(current)
                flow_nodes.add(current)
                
                # Follow data flow relationships
                for successor in self.graph.successors(current):
                    edge_data = self.graph.edges[current, successor]
                    relationship = edge_data.get("relationship", "")
                    if relationship in ["calls", "uses", "returns", "passes_to"]:
                        queue.append((successor, depth + 1))
        
        subgraph_data = self._extract_subgraph_data(list(flow_nodes), depth=0)
        subgraph_data["metadata"] = {
            "search_type": "data_flow",
            "start_node": start_node,
            "end_node": end_node,
            "flow_nodes": len(flow_nodes),
            "center_nodes": list(flow_nodes)
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    def find_security_hotspots(self) -> GraphGenerator:
        """
        Find potential security hotspots in the code.
        
        Returns:
            GraphGenerator subgraph containing security-sensitive code
        """
        security_keywords = [
            "password", "token", "secret", "key", "auth", "login", "credential",
            "jwt", "encrypt", "decrypt", "hash", "salt", "session", "cookie"
        ]
        
        security_nodes = []
        
        for node_id, node_data in self.nodes_map.items():
            node_name = node_data.get("name", "").lower()
            node_code = (node_data.get("code", "") or "").lower()
            node_file = (node_data.get("file", "") or "").lower()
            
            # Check for security-related keywords
            if (any(keyword in node_name for keyword in security_keywords) or
                any(keyword in node_code for keyword in security_keywords) or
                any(keyword in node_file for keyword in security_keywords)):
                
                security_nodes.append({
                    "node_id": node_id,
                    "security_score": sum(1 for keyword in security_keywords 
                                         if keyword in node_name + node_code + node_file)
                })
        
        # Sort by security relevance
        security_nodes.sort(key=lambda x: x["security_score"], reverse=True)
        center_nodes = [node["node_id"] for node in security_nodes]
        
        subgraph_data = self._extract_subgraph_data(center_nodes, depth=1)
        subgraph_data["metadata"] = {
            "search_type": "security_hotspots",
            "hotspots_found": len(security_nodes),
            "security_details": security_nodes[:20],  # Top 20
            "center_nodes": center_nodes
        }
        
        return self._create_subgraph_generator(subgraph_data)
    
    @staticmethod
    def combine_subgraphs(*subgraphs: GraphGenerator) -> GraphGenerator:
        """
        Combine multiple subgraphs into one.
        
        Args:
            *subgraphs: Variable number of GraphGenerator subgraphs
        
        Returns:
            GraphGenerator containing the union of all subgraphs
        """
        if not subgraphs:
            return GraphGenerator([])
        
        # Combine all nodes and edges
        all_nodes = []
        all_edges = []
        combined_metadata = {"search_type": "combined", "source_subgraphs": []}
        
        for subgraph in subgraphs:
            all_nodes.extend(subgraph.all_nodes_data)
            all_edges.extend(subgraph.all_edges_data)
            
            if hasattr(subgraph, '_search_metadata'):
                combined_metadata["source_subgraphs"].append(subgraph._search_metadata)
        
        # Remove duplicates
        unique_nodes = {}
        for node in all_nodes:
            unique_nodes[node["id"]] = node
        
        unique_edges = []
        edge_set = set()
        for edge in all_edges:
            edge_key = (edge["source"], edge["target"], edge.get("relationship", ""))
            if edge_key not in edge_set:
                unique_edges.append(edge)
                edge_set.add(edge_key)
        
        # Create combined subgraph
        combined = GraphGenerator([])
        combined.all_nodes_data = list(unique_nodes.values())
        combined.all_edges_data = unique_edges
        combined.node_details_map = unique_nodes
        combined._search_metadata = combined_metadata
        
        return combined
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about the original graph.
        
        Returns:
            Dictionary containing various graph statistics
        """
        if not self.nodes_map:
            return {}
        
        # Count nodes by category
        category_counts = defaultdict(int)
        for node in self.nodes_map.values():
            category_counts[node.get("category", "unknown")] += 1
        
        # Count edges by relationship type
        relationship_counts = defaultdict(int)
        for edge in self.edges_list:
            relationship_counts[edge.get("relationship", "unknown")] += 1
        
        # Graph connectivity stats
        if self.graph:
            strongly_connected = len(list(nx.strongly_connected_components(self.graph)))
            weakly_connected = len(list(nx.weakly_connected_components(self.graph)))
        else:
            strongly_connected = 0
            weakly_connected = 0
        
        return {
            "total_nodes": len(self.nodes_map),
            "total_edges": len(self.edges_list),
            "node_categories": dict(category_counts),
            "relationship_types": dict(relationship_counts),
            "strongly_connected_components": strongly_connected,
            "weakly_connected_components": weakly_connected,
            "average_degree": (2 * len(self.edges_list)) / max(len(self.nodes_map), 1)
        }